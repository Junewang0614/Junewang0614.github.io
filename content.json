{"meta":{"title":"June","subtitle":"","description":"曲径通幽处，禅房花木深","author":"June wang","url":"http://junewang0614.github.io","root":"/"},"pages":[{"title":"board","date":"2020-01-17T01:58:59.000Z","updated":"2020-08-11T14:22:17.420Z","comments":true,"path":"board/index.html","permalink":"http://junewang0614.github.io/board/index.html","excerpt":"","text":""}],"posts":[{"title":"机器学习笔记-11","slug":"机器学习笔记-11","date":"2020-09-11T08:33:15.000Z","updated":"2020-09-11T08:46:18.982Z","comments":true,"path":"2020/09/11/机器学习笔记-11/","link":"","permalink":"http://junewang0614.github.io/2020/09/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-11/","excerpt":"","text":"Structured support vector machine","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-10","slug":"机器学习笔记-10","date":"2020-09-11T07:02:18.000Z","updated":"2020-09-11T08:31:21.634Z","comments":true,"path":"2020/09/11/机器学习笔记-10/","link":"","permalink":"http://junewang0614.github.io/2020/09/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-10/","excerpt":"","text":"Structured Learning 结构化学习1、应用举例 语音辨识 翻译 句法分析 syntactic parsing 对象检测 object detection等等 Unified Framework 统一框架 主要分为training训练和inference推断两步： training部分目的是找到F(X,Y)，训练数据x与y都是输入值，F的输出值是一个评判值，当x与y越匹配的时候，F得到的输出值越高，F是评估x与y匹配的一个函数。 inference 推断：将未知的X值输入，然后穷举所有可能的Y，将X与Y带入训练好的F函数中，使得F函数值最高的Y，即我们要找的输出。 以对象检测为例输入：一张图像输出：想要寻找的对象所在的位置 Training 训练部分如上图，将图像X与框框Y随机匹配，当Y框对了对象的时候，F的值最高，当Y框了部分对象的时候，F的值偏低，当Y完全没有框对对象时，F的值最低。我们要训练的就是这样的一个F函数。 Testing输入没有接触过的图像x，穷举所有的框框Y，依次与X一起放入训练好的F函数，进行匹配，当F越高，说明Y框框正确的可能性越大（或者框出的部分中，正确的部分占比越大），取使得F最大的Y，就是我们需要的输出。 统一框架中的三大问题以及解决方法 以结构化线性模型为例 Problem-1：F函数是一个什么样的函数 Problem-2：怎么做，才能穷举所有的Y，得到使得F函数最大值的Y，即如何解决最大化问题假设问题已经被解决了 Problem-3：如何通过训练找到F函数由第一个问题可得，解决这个问题的实质是，找到满足什么样的条件的W向量。满足的条件如图的黄色部分所示： 即：w与正确的(x,y)对的特征做内积的值，都会大于任意的W与其他的(x,y)对做内积的值。 举例说明：其中红色的点是正确的点（training data),蓝色的点是任意点。根据内积的几何意义，也就是将每一个(x,y)对，投影在W向量方向上，要满足所有红色的同类型的点，在W向量方向上的值是最大的。需要根据training data训练出这样的W向量 解决方案演算法 主要步骤： 初始化W为0 对于每一对training data,穷举所有的Y，然后将对应的(x,y)特征与目前的W做内积，选出使得这个内积最大的y’。（这个其实是第二个问题，现在我们假设了第二个问题已经被解决了，也就是我们可以计算出最大值，以及找到对应的y） 如果y’ != 正确的y，按照图片上的式子更新W的值，再进入训话计算。 直到对于训练集中所有的(x，y)对，使得W与特征的内积最大的就是正确的y的时候，W不再更新，跳出循环，训练完成。","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-09","slug":"机器学习笔记-09","date":"2020-09-10T10:34:00.000Z","updated":"2020-09-11T07:01:59.807Z","comments":true,"path":"2020/09/10/机器学习笔记-09/","link":"","permalink":"http://junewang0614.github.io/2020/09/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-09/","excerpt":"","text":"Unsupervised Learning 无监督学习 化繁为简 输入比较复杂的input，得到比较简单的output 拥有的训练data，只有input，不知道output 无中生有 训练某种function，训练集里只有function的output Clustering 聚类化繁为简的好方法 K-means（K均值）：将X集合里的元素聚类为K个类别。 HAC(hierarchical Agglomerative Clustering)凝聚层次聚类：利用树的结构表示Data在树的不同层次划分，可以得到不同数量的class。 Dimension Reduction 降维从比较高维的空间变为比较低维的空间。实质是输入一个x，经过某一function，得到输出z，z的维度小于x的维度。 方法： Feature selection 特征选择 Principle component analysis(PCA) 主成分分析 （未完待续）","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-08","slug":"机器学习笔记-08","date":"2020-09-09T12:36:58.000Z","updated":"2020-09-10T10:38:05.210Z","comments":true,"path":"2020/09/09/机器学习笔记-08/","link":"","permalink":"http://junewang0614.github.io/2020/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-08/","excerpt":"","text":"Semi-supervised Learning 半监督学习基本介绍 半监督学习的特点是，训练数据中存在一部分没有标记的数据，且数据量一般比标记后的数据要大。 为什么使用半监督学习？因为我们更多的数据是没有标记的，如果能有效地利用这些未标记数据可以将模型训练的更好，但是这方面，精确的“假设”起很重要的作用。 generative model 生成模型1、初始化模型参数后，计算每一个没有标记数据的后验概率，看看是输入哪一类别。 2、利用已经得到的数据，再次更新模型的参数（协方差、均值等） 3、反复第一步第二步，理论上模型最后会收敛。 self-trainingEntropy-based Regularization1、我们通过标记数据的得到的模型，用来计算未标记的数据，这时候得到的对应的y是一些分布（distribution），这个分布越偏向某一类是越好的分布。判断分布的好坏可以利用信息熵来计算。信息熵为0是好的，信息熵很大的是不好的。 2、根据这个假设可以重新设置loss函数。 全新的loss函数由两部分组成：第一部分对应的是labelled data，计算模型中的y值和实际的y值的距离；第二部分对应的是unlabeled data，计算信息熵让其最小化。 Smoothness Assumption(平滑的假设)假设：对于相似的X，Y值假设是一样的详细：1、x是不平均的，在某些地方集中，在某些地方分散。2、当x1,x2在高密度的地方是接近的，y1与y2假设是一样的。 x1,x2更偏向于有相同的y。 Graph-based Approach通过图的连通性，反映两个点之间的相似性。 将数据用图的形式表示出来 labelled data会影响自身连接的邻居值。 被影响的数据点，也具有影响性，可以影响其他与其相连的点，即使这个点没有和任何的labelled data相连。 定量表示smoothnesss的值越小，表示越光滑 根据该假设又可以重新定义Loss函数 Better Representation 化繁为简比较复杂的事物是由比较简单的事物操控，透过现象看到本质以后能进行更好的训练。","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-07","slug":"机器学习笔记-07","date":"2020-09-07T05:19:10.000Z","updated":"2020-09-09T12:35:37.023Z","comments":true,"path":"2020/09/07/机器学习笔记-07/","link":"","permalink":"http://junewang0614.github.io/2020/09/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-07/","excerpt":"","text":"Recurrent Neural Network(RNN)对应视频：p20-p21 基本架构Elman Network 橙、绿、黄构成一个前馈神经网络，蓝色保存了中间层的输出内容。 三个前馈神经网络是同一个神经网络，使用时间不同，输入的蓝色部分的内容也因此不同。蓝色的存储元件是循环神经网络的重点。 构成深度神经网络也可以，每一层的输出内容都会被保存下来。 其中蓝色箭头表示每一次存储下来的内容，第一次是需要为其赋初始值。 Elman Network的特点是，保存的是每一个隐藏层的输出值 Jordan Network jordan network的特点是，把每一次的真实输出值保存下来，作为下一次的输入部分 Long Short-term Memory（LSTM）1、构成 Memory Cell：存储内容 Input Gate：某一内容能否保留在Memory Cell中，需要操纵信号 Output Gate: 外界可不可以得到当前保存的内容，需要操纵讯号 Forget Gate：将memory Cell中的内容忘掉，需要操纵讯号 其中后面三个Gate什么时候打开什么时候关闭都是由机器自主学习决定。蓝色箭头是四个输入：input gate信号、input的内容，output gate信号，forget gate信号红色箭头是一个输出：output输出。 2、工作流程 原来的cell中保存的内容为C 其中所有为f的函数，都是sigmoid函数，因为可以将输入的值变换后控制在0-1之间，0代表门关闭，1代表门打开。 g，h函数则是对输入的内容进行一定的变换。 最终的输出为a 第一步输入z，经过g函数得到g(z)，这是想要保存的内容。经过f函数得到f(zi)，这是判断能否输入的标志，如果为0则不能输入——相当于输入的值为0. 第二步经过f函数得到f(zf)，这是forget gate的标志。不同的是，如果值为0，代表cell中的内容被清洗了，而如果值为1，cell中的内容依旧保留着。 第三步，经过输入后，重新保存在cell中的c’公式为： c’ = g(z)f(zi)(新输入的部分) + cf(zf)(之前保存的部分) 第四步，保存在cell中的内容经过h函数变换为h(c’)，这是可能将会输出的内容，经过f函数得到f(zo)，如果为0，则数据不会输出——相当于输出值为0。 最后实际输出值a = f(zo)h(c’) 图上所有f的部分，代表的都是一些讯号。 3、网络构成 理论基础流程 实际 RNN学习问题梯度爆炸梯度消失通过LSTM可以解决 原理解释：LSTM中memory的更新，是旧值+新值，在forget gate不工作的时候，旧值不会被刷新，旧值的影响始终是存在的。 RNN应用 输入序列，得到一个结果（many to one） sentiment analysis情感分析 many to many（output &lt; input) 语音辨识 speech recognition CTC：Connectionist Temporal Classification,核心是加一个NULL的标识输出，解决叠字问题。 sequence to sequence（no limitation） 机器翻译 Syntactic parsing 语音信号可以转换为向量 聊天机器人 其他Attention-based Model","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"pythorch","slug":"pythorch","date":"2020-08-03T11:42:27.000Z","updated":"2020-08-03T16:19:50.768Z","comments":true,"path":"2020/08/03/pythorch/","link":"","permalink":"http://junewang0614.github.io/2020/08/03/pythorch/","excerpt":"","text":"PyTorch Introductiontensor张量1、与numpy的相互转换 1234#tensor-&gt;numpy ：y_numpy = torch.numpy(x_tensor)#numpy-&gt;tensor ：y_torch = torch.from_numpy(x_numpy) 2、tensor可以转变矩阵形状，只要总共的元素数不变就行 123456#tensor.viewx = torch.rand(5,3)y = x.view(15)z = x.view(-1, 5) # -1所指的维度可以根据其他维度的值推出来#只有一个维度可以赋值-1！print(x.size(), y.size(), z.size()) 3、","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"CS224n-01","slug":"CS224n-01","date":"2020-08-03T11:11:53.000Z","updated":"2020-08-03T16:19:45.055Z","comments":true,"path":"2020/08/03/CS224n-01/","link":"","permalink":"http://junewang0614.github.io/2020/08/03/CS224n-01/","excerpt":"","text":"词向量表示bilibili —p2 nltk","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"《神经网络与深度学习》第一章 阅读笔记","slug":"NNDL阅读笔记-第二章","date":"2020-08-02T07:26:35.000Z","updated":"2020-08-02T10:30:12.525Z","comments":true,"path":"2020/08/02/NNDL阅读笔记-第二章/","link":"","permalink":"http://junewang0614.github.io/2020/08/02/NNDL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/","excerpt":"第二章 机器学习概述","text":"第二章 机器学习概述 基本概念1、特征 or 属性：x，特征向量 2、标签 y 3、数据集、训练集、测试集 4、函数 f 三要素 机器学习算法类型 评价指标1、准确率，错误率 准确率+错误率 = 1 2、精确率和召回率（对每个类进行性能估计） 模型在测试集的结果分为以下四类： 真正例：TP，实际为c，预测为c 假负例：FN，实际为c，预测为其他 假正例：FP，实际为其他，预测为c 真负例：TN，实际为其他，预测为其他 精确率：所有预测为类别c(P)中正确的比例：TPc/(TPc+FPc) 召回率：所有真实类别为c中正确的比例：TPc/(TPc + FNc) 3、F值：综合指标，召回率和精确率的调和平均 4、宏平均：每一类性能指标的算术平均、微平均：每一个样本的性能指标的算数平均 5、交叉验证：原始数据集均分为K组不重复子集，每次选择k-1作为训练集，剩下一组作为验证集。从而进行K次训练，得到K组模型，将k个模型在各自验证集上的错误率的平均作为分类器的评价。","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-06","slug":"机器学习笔记-06","date":"2020-08-02T06:59:33.000Z","updated":"2020-08-03T10:46:30.700Z","comments":true,"path":"2020/08/02/机器学习笔记-06/","link":"","permalink":"http://junewang0614.github.io/2020/08/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-06/","excerpt":"Convolutional Neural Network(CNN)简化整体网络结构，选择较少的参数（相较于全连接前馈网络）。 bilibili P14","text":"Convolutional Neural Network(CNN)简化整体网络结构，选择较少的参数（相较于全连接前馈网络）。 bilibili P14 影像处理为例Q1：为什么可以将全连接网络里面的一些参数去掉，简化网络？ A：1、每一个neural都类似于一个分类器，用以判断某些特征，特定特征的判断需要的是相对应的输入，部分的输入内容就可以决定相关特征而不需要利用所有输入进行判断。2、相同的特征判断可能存在于输入的不同位置（比如两张图片中同是鸟嘴但位置不同），但此时使用相同的的neural就好，而不需要两个neural，可以让两个neural相互share参数，共用同一组参数工作。3、subsampling（比如缩小）对图像的判断可能没有影响，可以通过二次抽样来减少参数的使用。 CNN全过程 ConvolutionQ1：Convolution做了什么？ A：（以影像处理为例），convolution做了第1、2部分。通过卷积的方式，将不同位置的相同的特征，借助filter过滤器，提取出不同的特征图谱。 eg：过滤器在右上角，是33的矩阵。过滤器每次取图片的一部分（大小相同的一部分）做内积，得到一个结果，然后移动固定的步数（这里是1），继续做内积，最后得到了新的4\\4的矩阵。多个filter做相同的事情，得到很多矩阵，合在一起成为了特征图谱。 Q2：convolution和全连接网络的关系，以及why？ A：convolution的实质，就是全连接网络某一层中，去掉一些weight的结果。输出的特征矩阵的每一个值，相当于网络中某一个神经元的输出值。全连接网络中每一个神经元得到的输入是全部的输入，convolution中输入只是与过滤器做内积的那部分选中的输入。而过滤器矩阵的值其实就是weight值，是机器可以自己学习出来的内容。 部分的输入带来了较少的参数值，需要自主学习的参数的数量减少了。 16个神经元，只是每个神经元的输入对应input的不同的部分，使用的过滤器相同–&gt;每次计算所利用的权值相同–&gt;shared weights–&gt;又可以减少参数的数量。 Max Poolingconvolution的输出矩阵，将矩阵按照一定规则划分，并按照一定规则在每个划分区域进行计算或者选择，使每个区域保留一个数值，生成新的矩阵。 阶段总结 经过一次convolution+max pooling，原本是6*6的图片特征变成了2*2的特征图谱。新的特征图谱可以作为input参与下一次的convolution+max pooling，这一套可以重复很多次，每一次都会缩小矩阵大小。 其他 剩下的两步：flatten相当于是把立体的特征图谱矩阵拉成多维向量，作为全新的input输入到全连接网络，全连接网络进行计算，output，结束。 深入探究（以图像识别为例） filter每一个filter的作用是什么？ 方法：在model已经训练好，参数全部固定的情况下，不断调整输入的x的值（输入的图片），要调整到被观测的filter的活跃度最高，其中活跃度的定义为通过该过滤器输出的矩阵的每一个值的总和。 结果发现每一个filter的作用是检测某种类型的线条走向，例如纵线条、横线条、不同方向的斜线条等。 neural最后的全连接神经网络中每一个neural的作用是什么？ 方法：（类似），调整输入x，最大化当前neural的输出值。 结果：侦测的是比较完整的一个图像，与filter不同的是，filter的输入只是图像中很小的一部分内容，侦测的也只是那一部分的特征，而全连接神经网络每一个neural都是输入所有的input，所以反应的是要侦测的整个图像的特征。 output layer最后的输出层的作用是什么？ 方法：方法同理 结果猜想应该是数字的样子。但是最后的结果并不是这样，每一种输出都是相似的混乱的图片。说明机器最终学习到的和人类还是很不一样的内容，虽然都能对图像作出判断，但是其中的原理有很大区别。 调整 方法：使得输出y值最大的同时，也要让输入的x最小（黑白图中，x越大表示笔画部分越多，空白部分越少）。 好玩的其他研究应用：deep dream，deep style CNN在下围棋上表现比普通的全连接网络表现更好。（why？） 围棋图谱有和图像识别相类似的地方，也就是适合CNN处理的地方 图谱中也有很多特征（基本的pattern），远小于整个图谱的大小。 很多相同的特征（pattern），可以出现在不同的位置，代表同样的意义。 Alpha Go的CNN结构的特别之处：没有使用Max Pooling，因为Max pooling 不需要被使用在围棋处理架构中。（第三个特征不符） Speech应用，将音频图片输入到CNN，同样是对图片的处理。 text，文字处理应用，同样类似于处理图像 感觉CNN的实质还是针对于图像处理的那三个特点，这样的表现有很好的效果。其他方面的应用也是有这方面的原理在，是三个特点的变形使用。","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-05","slug":"机器学习笔记-05","date":"2020-08-02T04:14:14.000Z","updated":"2020-08-02T10:29:51.168Z","comments":true,"path":"2020/08/02/机器学习笔记-05/","link":"","permalink":"http://junewang0614.github.io/2020/08/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-05/","excerpt":"Backpropagation——反向传播是有效计算gradient的向量的方法，因为随着网络的层数的增加，参数越来越多，向量越来越长，计算越来越复杂，使用反向传播比较高效。","text":"Backpropagation——反向传播是有效计算gradient的向量的方法，因为随着网络的层数的增加，参数越来越多，向量越来越长，计算越来越复杂，使用反向传播比较高效。 预备知识chain rule 链式法则用于多变量求导 σ(z)函数 backpopagation过程 梯度下降的方式是用于求得Loss函数的最小值，loss函数是一个求和函数，取决于每一个l的值，所以重点在于计算w参数对C的微分。 (其中C就是上图的小l) 求w对C的微分，可以通过链式法则分解，引入中间变量z。求w对z的微分是forward pass，有图可得，w1对z的偏微分就是x1,可以以此类推，wn对z的偏微分，就是wn前面的input值。求z对C的微分是backward pass Forward pass正如前面所述，wn对z的偏微分，就是wn前面的input值。一定是当层的、最接近的input的值，可以是来自外部输出，也可以是来自前一层网络。 Backward pass考虑的是下一步发生了什么事情。 引入中间变量a，使得a = σ(z) 可以通过链式法则展开，得到z对C的偏微分。其中可以看到关系可以普遍简化定义为:a会影响z’，z’’(可能还有别的z)，z’,z’’会影响c，从而有a影响了c，通过链式法则可以表示出c对a的偏微分。 第一项a对z’等的偏微分都很好算，a是作为input的层，通过forward pass可以知道，对应的是w1,w2……wn,后面那一项我们无从下手。 假定我们已经知道后面那一项的值，则整个式子可以进行一定的整理 如何计算未知的两项？ – 分类讨论 case 1:已经到达了output layer case 2 Not output layer 通过递归的方式计算，直到打倒了output 层，然后可以回溯到前面层 实际计算方式 computer z对c的偏微分 from the output layer,从输出层向后计算，计算有效率。 实际是，建立一个反向的神经网络，来计算整个的偏微分。 Summary 先计算ForWard pass 再通过逆向网络计算 Backward pass 两者相乘，最终得到了w对C的偏微分","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"《神经网络与深度学习》第一章 阅读笔记","slug":"NNDL阅读笔记-第一章","date":"2020-07-26T07:34:47.000Z","updated":"2020-07-26T11:56:36.824Z","comments":true,"path":"2020/07/26/NNDL阅读笔记-第一章/","link":"","permalink":"http://junewang0614.github.io/2020/07/26/NNDL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/","excerpt":"第一章 绪论基本概念一览","text":"第一章 绪论基本概念一览 人工智能人工智能就是让机器人具有人类的智能。 就是要让机器的行为看起来就像是人所表现出的智能的行为一样。 因为“智能”较难定义，判断机器是否智能的方式则采用“图灵测试”。 人工智能主要领域划分： 感知（语音信息处理、计算机视觉等） 学习（监督学习、半监督、无监督学习等） 认知（知识表示、自然语言理解、推理等） 人工智能发展历程：推理期（规律较简单） — 知识期（知识库、专家系统）— 学习期（计算机从数据中自主学习） 人工智能流派（主要）：符号主义与连接主义。 机器学习（Machine Learning)机器学习是指从有限的观测数据中学习出具有一般性的规律，并利用这些规律对未知的数据进行预测的方法。 机器学习模型一般包括以下步骤：数据预处理 – 特征提取 – 特征转换 – 预测其中预测是机器学习的核心部分，机器需要自主学习到一个函数（模型），来进行未知数据的预测 传统的机器学习，实际操作过程中，数据预处理、特征提取、特征转换反而需要人类干预，好的特征影响着最终模型，所以有时机器学习的主要工作量在特征问题上。这其实是一种“浅层学习” 表示学习(Representation Learning)表示，或称之为特征表示学习指可以自动的学习出有效的特征，并提高最终机器学习模型的性能的学习。 表示学习两个核心问题：什么是好的表示？ 如何学习到好的表示？ 好的表示的优点： 很强的表示能力 使后续的学习任务变得简单 具有一般性，可以比较容易迁移到其他任务上 常用表示 局部表示 用一个很长的向量表示特征，有则为1，无则为0 优点：很好的解释性；稀疏的二值向量，计算效率很高 不足：向量的维数很高，不易扩展（扩展就要增维）；不同特征之间的相关性都相同，不能很好表示特征间的关系。 分布式表示 向量维度低，方便扩展，相似度也可以计算 其他嵌入：将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系。（由高维空间映射到低维空间） 深度学习(Deep Learning)“深度”：指原始数据进行非线性特征转换的次数 深度学习的目的是从数据中自动学习到有效的特征表示（之前都是人工设计特征偏多） 深度学习需要解决问题：贡献度分配即一个系统中不同的组间或其参数对最终系统输出结果的贡献或影响。 端到端学习端到端学习指在学习过程中，不进行分模块或分阶段训练，直接优化任务的总目标，端到端学习的中间过程不需要人为干预。训练数据为“输入-输出”对的形式而不需要提供其他信息。 深度学习算是一种端到端学习 神经网络( Neural Network)神经网络是指由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数。 详细内容还需要通过后面的学习认识。 发展历史：模型提出 – 冰河期（反向传播算法提出被忽略）– 反向传播算法引起复兴（梯度消失问题出现）– 流行度降低 – 深度学习的崛起（以神经网络为基础）","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-04","slug":"机器学习笔记-04","date":"2020-07-26T02:20:51.000Z","updated":"2020-08-02T04:13:25.115Z","comments":true,"path":"2020/07/26/机器学习笔记-04/","link":"","permalink":"http://junewang0614.github.io/2020/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-04/","excerpt":"Deep Learningbilbil：P12、P14 三步走： step1：确定网络的链接方式，确定网络结构一种连接方式可以对应一个function set","text":"Deep Learningbilbil：P12、P14 三步走： step1：确定网络的链接方式，确定网络结构一种连接方式可以对应一个function set step 1链接方式介绍： fully connect feedforward network（全连接前馈网络） 特点： input传递是从低级的layer向高级的layer传递，所以是forward 对于低级layer的每一个神经元（neuron）得到的对应的output结果，要传给下一层的每一个neuron，这则是所谓的Fully，全连接。 Deep = many hidden layers（很多的隐藏层） 一个layer的运算可以通过一个矩阵表示。整个全连接网络可以用矩阵的计算表示，好处在于矩阵运算可以通过GPU加速。里面的正方形是矩阵长方形也是，但同时也是一个向量。 hidden layers中，主要是将原始的feature进行一定的变换，使得在最后的output层可以比较容易的得到好的模型 Q:如何决定层数、决定每层的神经元数、决定链接方式？（好的结构里面存在好的function） A:直觉、经验、尝试等 step 2定义function的好坏，利用training data，同逻辑回归的方式。利用梯度下降的方式，最小化Loss函数，得到相应的function和它的参数集。 step 3选择好的function WHY DEEP?问题：为什么是增加层数layer，而不是在中间层不断增加神经元？实质是在参数相同的情况下，层数更多的（thin）网络结构表现更好（deep），还是一层内神经元更多的（fat）网络结构表现更好？ 结果表明，层数更多比单单一层更多神经元的结构表现效果好很多。原因：deep是在模块化网络。模块化的好处是能够减少冗余、重复利用。对于deep learning 来讲， 效果则是可以用比较少的data也能到的好的模型。底层的layer相当于basic classifier，高层的模型建立都是基于底层。模块化是机器自动从data学习而来。（？） 举例语音中的模块化运用语音辨识初步过程1、取语音音频，每次截取一小段并将其归类为不同的state2、每一个state要进行分类，分给不同的phoneme（发音单位）（这里就用到了分类）3、其中因为phoneme与前后的发音有关系，又将相同的phoneme分成不同的tri类，然后每个tri类又分为三份，理论上每一份对应一个model。问题来了，这样的话model太多了，数据量根本训练不出好的模型。 发现：不同的“每一份”可以共用相同的分布模型。 —- 模块化 其实phoneme之间客观上也是有关系的。 deep learning做法： input：acoustic feature output：probability of each state 所有的states共用同一个DNN（深度神经网络） 先研究发音方式（模块化），然后进行发音分类 DNN的好处（模块化） 使用参数更有效率 普遍性定理是，一层layer就可以将各种参数转换成想要的维度的输出，一层网络就可以表示所有的function，但是这样矮胖的网络结构是没有效率的。深层网络是比较有效率的。 逻辑电路类比1、两层逻辑电路可以表示任何的boolean function。2、但没有人会这么做，因为没有效率，多层结构比较有效率，可以用比较少的逻辑门。 更少的逻辑门象征更少的参数需要计算，即降低overfitting的概率逻辑电路中的道理和和深度神经网络是一样的最终的效果是，较少的data可以训练出类似的结果 END TO END learning只给机器input 与 output，让其自己进行学习训练。 理论上的想法： 其中的function不会经过人工加工，不通过人工的之前的研究、经验，直接丢给机器，让其自主进行数据分析，功能完善。后来通过谷歌的研究发现，机器的再自主学习做出的最终结果与人类干预差不多，不会比人类知识干预要好，分析发现机器所做的学习和人类通过认知进行的数据干预处理是类似的。 Deep learning的其他好处 有很多复杂的task，可能有的输入很类似，但是输出的结果应当是不同的；有的输入很不一样，但是输出的结果应该是相同的。想要达到这种效果，就需要让数据经过很多层次的转换。deep learning就可以实现这样的事情。","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-03","slug":"机器学习笔记-03","date":"2020-07-19T09:08:03.000Z","updated":"2020-07-26T07:29:57.576Z","comments":true,"path":"2020/07/19/机器学习笔记-03/","link":"","permalink":"http://junewang0614.github.io/2020/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-03/","excerpt":"Classification 分类1、理论方法：三步走 概率知识：贝叶斯公式（后验概率），高斯分布 伯努利分布 == 二项分布 高斯分布 == 正态分布","text":"Classification 分类1、理论方法：三步走 概率知识：贝叶斯公式（后验概率），高斯分布 伯努利分布 == 二项分布 高斯分布 == 正态分布 一般过程假设一个机率模型 1、training data 估测一个gaussian distribution（高斯分布结果），通过高斯分布计算出新的x在范围内被选择出来的几率。 2、不同的高斯分布对应不同的likelihood（可能性），找到分布中，面对training data数据，标记概率（likelihood）是最大的，选择这样的分布，得到相应的参数。 穷举各种μ与ξ的值，然后通过图片方法计算，选择最大可能性的一组参数 3、每个class都要计算第二步，得到相应的（μ，ξ)。 4、分类计算，计算输入的x来自于不同的class的几率 5、效果不好的时候（比如二维空间分类分不出来），可以提高feature的维度，即增加输入向量的长度。（最后计算效果不好！！:triumph:） 改良处理1、减少模型参数，参数越多越容易overfitting，让不同的class共用一个ξ，使得likelihood最大。 计算参数方式：μ的计算同上，ξ的计算不同，需要加权计算一下，likelihood函数也不同,如图下 相同的ξ，让boundary变成了直线，这也是linear model。 计算后得出准确率高了很多。 阶段总结——概率模型做（二）分类 相关数学知识（扩展）1、linear model的导出过程： logistic Regression回顾分类设定的模型： 这是一个逻辑回归函数 2、通过训练集，选择出最合适的w,b参数：定义function的好坏 L是发生训练集所展现分类的概率函数，假设每个元素属于某一类都是相互独立的，所以计算L的方式是将计算出的概率（f函数）相乘。让L最大值所对应的w,b的值，就是我们模型中需要找出的w,b的值。 ++++++++++++++++++++ 下面是数学相关的L函数的化简整理 理解这几条： 因为L是累乘的结果，为了方便计算，取ln变成累加，再整体取符号，化取最大值为取最小值。 引入y^，（二分类中）y取1表示输入class1，取0表示属于class2. 将每一项的形式都通过y^的引入而同一成相同形式 统一出来的相同形式是两个二项分布的交叉熵，其中交叉熵越接近0代表两个分布越相同，我们想要的就是交叉熵尽可能接近0. 以下是ppt讲义部分： 其中y的取值只有1与0，f因为是个概率函数取值在0-1之间，所以方括号内的部分一定是小于零的，加上外面的负号会大于0，所以方括号越趋近于零，一个是代表两个分布越接近，另一个也是代表-lnL的值趋向最小，表示模型越来越匹配，所以交叉熵越趋近于0越好。 下面的H是交叉熵的计算方法。 于是我们可以得到逻辑回归（二分类）的判定模型好坏的函数L。 逻辑回归与线性回归的对比 Q:为什么不能直接使用线性回归的L函数（square error）当作逻辑回归的L函数，线性的相对而言更简单易算？ A：利用梯度下降的方式逼近最合适的w的时候，会出现无法区别距离实际值到底很近还是很远的状况，在梯度下降的时候会卡住，不容易得到好的结果： 3、通过Loss函数计算最佳w，b的方法：数学问题，对Loss函数对w求偏微分，通过一系列变换，得到一个直观的结果。然后通过梯度下降的方法，求得loss的最低点w的值 总结对比 生成模型和判别模型对比（有些不懂）（暂时理解）生成模型是最初的分类模型，计算ξ、μ参数的那个模型（假定过分布服从正态分布） 判别模型就是上面的概率模型，直接计算w，b参数的模型。 两个模型实质是相同的，（w,b),(μ，ξ)也都有一定对应关系，但是计算出的结果会发现参数的结果不同，也就是求出的具体模型是不同的。原因就在于生成模型提前有过很多假设，也对模型的计算产生一定的影响。 生成模型（generative model）的好处是： training data很少的时候，生成模型受data的影响小，有自己的假设，有时反而表现很好 data可能本来存在问题的时候，因为受data影响小，所以可能表现更好。 判别模型的好处是： 受data影响大，所以数据足够多足够准确的时候，计算出来的模型更优。 多分类问题（三分类为例）（笔记记得稍微有点简单，先大体理解一下） 逻辑回归的限制因为回归依旧是一条直线，所以总会存在，根本无法通过一条直线将类别分离的情况。 解决方式：Feature Transformation将特征值转化eg: 问题：如果找到合适的transformation？ 回答：交给机器 进行多层的logistic regression，直到类别可以完全分开。然后将处理好的新的特征值，再进行逻辑回归，得到好的模型结果。 每一个逻辑回归又叫做“神经元”（neuron），整个网络是神经网络（neural network）","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"Machine_learning_作业1","slug":"Machine-learning-作业1","date":"2020-07-12T03:49:45.000Z","updated":"2020-07-12T03:51:16.351Z","comments":true,"path":"2020/07/12/Machine-learning-作业1/","link":"","permalink":"http://junewang0614.github.io/2020/07/12/Machine-learning-%E4%BD%9C%E4%B8%9A1/","excerpt":"","text":"","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"2020-07胡言乱语","slug":"2020-07胡言乱语","date":"2020-07-06T15:39:24.000Z","updated":"2020-07-06T16:10:01.658Z","comments":true,"path":"2020/07/06/2020-07胡言乱语/","link":"","permalink":"http://junewang0614.github.io/2020/07/06/2020-07%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/","excerpt":"我也不知道写点什么，想到哪里写到那里吧","text":"我也不知道写点什么，想到哪里写到那里吧 《关于追星？》 ​ 最近总在想，喜欢上次次子对自己而言，到底是怎么样的。我喜欢他们，因为他们正能量、努力、积极向上、虽然公司非常非常不给力但是他们都能一直坚持到现在，我喜欢他们追求梦想的坚定，喜欢他们之间如亲人一样的羁绊，喜欢他们有趣的性格，喜欢他们在一起始终天真如以往。我喜欢他们的理由，是我希望自己也能做到的地方。可是喜欢了他们，太多的时间花费在看他们的物料上，没有物料就反复看从前的物料，自己现实生活没有认真地度过，逃避的地方太多了，对太多东西都丧失了兴趣，我觉得反而与自己想要的背道而驰。喜欢他们真的很快乐，但是也要看到喜欢他们对自己日常生活的不好的影响。追星应该是日常生活的调味剂，或者像那个写论文的姐姐一样，只是休息时候的消遣，而不应该为了看物料，耽误了正经事情。这点我一定要意识到，我已经二十了，不应该这么不懂事了。 ​ 我知道现在让自己一下子断开很难，因为每次打开pad-&gt;打开b站-&gt;搜索seventeen，已经成为自己的一个习惯了，所以切断一切的源头，还是要戒手机、戒网络。所以，最近一阵儿手机先收起来，让自己心静下来，在慢慢地恢复正常的追星的感觉。记住自己的追星原则： 帅哥没有事业重要 白嫖的帅哥最美味 希望追星给自己带来的，是正能量，是自己喜欢的那种美好的东西。而不是成为自己逃避现实的老鼠洞。 《关于音乐》 音乐，是我觉得我真的很热爱的东西。如果热爱，就去行动吧，总是心痒痒的，很不好受吧。 好像难题解决了呢，感觉好容易呀哈哈哈，那就开始行动吧！","categories":[],"tags":[{"name":"嗯","slug":"嗯","permalink":"http://junewang0614.github.io/tags/%E5%97%AF/"}]},{"title":"机器学习笔记-02","slug":"人工智能学习笔记-02","date":"2020-06-27T11:09:45.000Z","updated":"2020-07-26T07:29:59.220Z","comments":true,"path":"2020/06/27/人工智能学习笔记-02/","link":"","permalink":"http://junewang0614.github.io/2020/06/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-02/","excerpt":"regression 回归linear model 线性模型1、设计模型 2、评价确定的参数是否合适，利用Loss function L，损失来评价 3、通过数据训练模型training data 4、testing data计算ERROR 挑选最好的function，也就是使得损失最小的function","text":"regression 回归linear model 线性模型1、设计模型 2、评价确定的参数是否合适，利用Loss function L，损失来评价 3、通过数据训练模型training data 4、testing data计算ERROR 挑选最好的function，也就是使得损失最小的function 计算方法：梯度下降Gradient Descent，寻找L（loss）的最低点 随机选取初始点W0 计算W0对L的微分，切线斜率，判断w0的值如何变动（取决于微分与learning rate），w0-》w1 反复执行第二步 最后找出的是局部最优解，也就是微分为0的位置。不一定全局最优（linear上不存在这个问题。没有局部最优解） 关于推广：一个参数推广到两个参数的时候，也就是偏微分的计算，与一个变量原理相同。 5、优化模型：再设计、再训练、再评价，模型由简单到复杂 model越来越复杂，training data 越来越匹配，但testing data表现不一定 ——》overfitting 过拟合，所以要选择合适的model 适当引入feature用于分类（？），可以使模型更匹配 Q:为什么我们更期待一个参数更接近0的function? A:比较平滑的，参数小可以让输入对于输出的变化不敏感。（？） 1、bias的影响：看结果集合与中心的偏离程度 （模型的复杂程度影响，越复杂，bias越小） 样本均值和总体均值的关系（？）———无偏估计 样本均值的期望等于总体均值———bias小 2、variance的影响：看结果集的分散程度（模型的复杂程度会影响variance,越复杂，variance越大) 样本方差和总体方差的关系（？）———有偏估计 简单model受到样本数据的影响比较小 确定一种model，相当于确定了一种范围，模型越复杂，范围越大，范围越大。点更容易分散 两者越低越好，寻找平衡 variance大：overfitting（training拟合好，但是test拟合不好）————增加data/regularization正则化 bias大：underfitting（training拟合不好）————重新设计model 更复杂：更多的feature，更高的阶层 Gradient Descent 梯度下降 过程： 其中 即为梯度，因为前面有了负号，所以点的移动总是向着梯度下降的方向进行，因而叫做梯度下降。 关于learning rate 太小：梯度下降处理速度慢 太大：直接错过min值 措施：关注前几次的参数updates，画图(如下图所示），可以提前发现太大的learning rate，当loss值可以稳定下降的时候，rate才相对合适 调整方法：可以将learning rate 成为t（次数）的函数。learning rate的变化规律一般为：值可以先很大，方便我们快速定位到最小值附近，然后值减小，慢慢靠近最低值，防止因为值过大错过最低值。 注意！learning rate也是要随着不同的参数而变化的，不是一成不变的，而且对于同一时间段的多种参数（如w，b等），learning rate 依旧有差别 Adagrad： g是对应参数关于x的微分或偏微分，所以η是关于下降次数(t)的函数，σ是关于对应参数微分(g)的函数，由此得出的learning rate可以和t与g相关 Stochastic只选择某一个example的Loss值，通过一个example进行梯度下降，得到相应参数。 好处：速度快，同样的起始点，可以update参数的次数变多了，计算内容少，更快。 Feature Scaling 特征缩放让不同的特征，有相同的比例，统一到一定的范围内。让每个参数对目标的影响程度相似。这样子learning rate也能够相对平稳一些。 方法： mean：均值，standard deviation：标准差 Theory 理论基础泰勒级数，泰勒展开","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"机器学习笔记-01","slug":"人工智能学习笔记-01","date":"2020-06-25T07:36:06.000Z","updated":"2020-09-11T08:32:07.618Z","comments":true,"path":"2020/06/25/人工智能学习笔记-01/","link":"","permalink":"http://junewang0614.github.io/2020/06/25/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-01/","excerpt":"概述Machine learning 三步骤 框架：","text":"概述Machine learning 三步骤 框架： 定义中英对照表 English Chinese Reinforcement learning 强化学习 Supervised learning 监督学习 Reinforcement learning 无监督学习 linear 线性 Network Architecture 网络架构 Gradient Descent 梯度下降 Adversarial Attack 对抗攻击 Explainable AI Network Compression 网络压缩 Anomaly Detection Transfer Learning 迁移学习 Meta learning 元学习 life-long learning 终身学习 Regression 回归 parameter 参数 feature 特征 optimal 最优的 convex 凸函数 overfitting 过拟合 generalization 一般化 regularization 正则化、规范化 bias variance estimator 估计、预估 validation set 验证集（training data的一部分） cross validation 交叉验证 optimization 最佳化、最优化 root mean square 均值平方根 prior vector 向量 gaussian distribution 高斯分布 maximum likelihood 最大似然 naive bayes 朴素贝叶斯 sigmoid function S型函数 logistic Regression 逻辑回归 cross entropy 交叉熵 Bernoulli Distribution 伯努利分布、二项分布 generative model 生成模型 Discriminative Model 判别模型 Neuron 神经元 neural network 神经网络 modularization 模块化 speech recognition 语音辨识 Backpropagation 反向传播 chain rule 链式法则 recursice 递归的、循环的 convolutional neural network CNN,卷积神经网络 subsampling 二次抽样 filter 过滤器 Feature map 特征图谱 feedforward 前馈 dimension 方面，维数 initial 最初的 gradient vanishing 梯度消失 gradient explode 梯度爆炸 neural turing machine 图灵机 Semi-supervised learning 半监督学习 Support Vector Machine（SVM） 支持向量机","categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"python学习日志","slug":"python学习日志","date":"2020-04-25T03:05:26.000Z","updated":"2020-04-25T03:14:25.340Z","comments":true,"path":"2020/04/25/python学习日志/","link":"","permalink":"http://junewang0614.github.io/2020/04/25/python%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/","excerpt":"python学习中遇到的知识总结一下 2020-04-25更新","text":"python学习中遇到的知识总结一下 2020-04-25更新 1、安装工具包 命令：python -m pip install packagename python -m pip install -U pip setuptools 更新pip和setuptools包 2、python交互式帮助系统help（） （1）查看模块 modules 查看计算机中所有模块 module name 输入模块名可以查看详细内容 module name. function name 句号表示法查看函数内容 modules module name 查看与该模块相关的模块 （2） dir(builtins)#两个下划线 查看内置函数列表 class 输入内置类名可以查看内置类 n、冷知识 （1） REPL：在控制台上交互执行python代码的过程（所以在sublime上配置的可以交互运行python的叫REPL啊。。 04-20新奇知识 1、动态函数 （1）eval：动态表达式的求值 （2）exec函数：动态语句执行 （3）compile函数：动态语句执行 2、函数式编程 （0）高阶函数：参数为函数对象的函数或返回函数对象的函数，称之为高阶函数，即函数的函数 f函数作为另一个函数的对象，是另一个函数的参数。 （1）map函数，返回可迭代对象 map(f,iterable,……)将f作用与可迭代对象中的每一个值，依次作用，并返回可迭代对象，f是函数。 （2）filter函数 过滤器 filter（f,iterable,……）返回可迭代对象，会根据f的作用，保留可迭代对象中f作用返回为true的值，返回可迭代对象。 （3）lamda表达式和匿名函数 在一行里定义一个函数，生成函数对象，即匿名函数 格式：lamda x1,x2，……：f(x1,x2,……) x1,x2是参数，f是对参数的做法 （4）sorted()函数：对可迭代对象进行排序 参数：key：排序法则，调用函数，导入函数对象 reverse：升序（False)降序(True)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://junewang0614.github.io/tags/Python/"}]},{"title":"<转>游走记忆的时间__中文填词","slug":"转-游走记忆的时间-中文填词","date":"2020-04-25T02:59:55.000Z","updated":"2020-07-04T08:11:31.975Z","comments":true,"path":"2020/04/25/转-游走记忆的时间-中文填词/","link":"","permalink":"http://junewang0614.github.io/2020/04/25/%E8%BD%AC-%E6%B8%B8%E8%B5%B0%E8%AE%B0%E5%BF%86%E7%9A%84%E6%97%B6%E9%97%B4-%E4%B8%AD%E6%96%87%E5%A1%AB%E8%AF%8D/","excerpt":"《游走记忆的时间》是我很喜欢的一首歌，在不懂得韩语歌词的情况下便被吸引。初听旋律会觉得很温柔很治愈，了解了歌词的含义后又会觉得有些悲伤。想尝试自己填一下中文歌词，现在文学素养和音乐乐理都有很大的不足，不过没关系，慢慢来，人还是要有梦想的。 这里有一个填的很不错的歌词，收藏下来学习一下。。","text":"《游走记忆的时间》是我很喜欢的一首歌，在不懂得韩语歌词的情况下便被吸引。初听旋律会觉得很温柔很治愈，了解了歌词的含义后又会觉得有些悲伤。想尝试自己填一下中文歌词，现在文学素养和音乐乐理都有很大的不足，不过没关系，慢慢来，人还是要有梦想的。 这里有一个填的很不错的歌词，收藏下来学习一下。。 中文填词 by 有趣的ACPV图片from：泰妍IG 转自b站 https://www.bilibili.com/video/BV1Ws411p7Rb?from=search&amp;seid=22808107266942372 还能够，听你的歌低着头还能够，假装握着你的手还不够，好想淹没在你褐色的眼眸 还能够，看着你对镜头说还能够，闻着你旧的枕头还不够，仿佛时间溜走像一个小偷 当我失去力气又漫无目的地在人海游走当我习惯压抑又刻意逃避只愿随波逐流当我嗅着晚风正刮过树叶那干燥的气息好像我的梦境它从未醒来，我还是拥有你 好想，回到很久以前，回到很久以前我知道你也会愿意回到很久以前 还能够，默出你的座右铭还能够，明明咫尺却隐忍舞台之后No，这样不够 每天都，看见你新的笑容每天都，听到你有多优秀，I know，我始终不能够原谅的优秀 当我失去力气又漫无目的地在人海游走当我习惯压抑又刻意逃避只愿随波逐流当我嗅着晚风正刮过树叶那干燥的气息好像我的梦境它从未醒来，我还是拥有你 好想，回到很久以前，回到很久以前但你的眼神告诉我唯恐避之不及 当你偶尔休息又重访故地在街边的长椅当**惯口渴却不常备着你爱喝的可乐当你无意之间走进了街边我在的便利店好像我的梦境它突然醒来，我从未拥有你 突然，唯恐避之不及，唯恐避之不及当你的眼神告诉我唯恐避之不及是你 啦啦啦唯恐避之不及啦啦啦唯恐避之不及 打开记忆被尘封的门锁，你的温柔疯狂地倒放着酸涩海水倒灌进我眼中打开记忆被尘封的门锁，你的渴望疯狂地倒放着酸涩海水倒灌进我眼中打开记忆被尘封的门锁，你的坚决疯狂地倒放着酸涩海水倒灌进我眼中打开记忆被尘封的门锁，你的一切疯狂地倒放着此刻的我已渐渐没有痛 啦啦啦已渐渐没有痛啦啦啦怎么会没有痛","categories":[],"tags":[{"name":"填词向","slug":"填词向","permalink":"http://junewang0614.github.io/tags/%E5%A1%AB%E8%AF%8D%E5%90%91/"}]},{"title":"python爬虫遇到的问题一览","slug":"python爬虫遇到的问题一览","date":"2020-04-15T02:20:10.000Z","updated":"2020-04-15T06:05:09.361Z","comments":true,"path":"2020/04/15/python爬虫遇到的问题一览/","link":"","permalink":"http://junewang0614.github.io/2020/04/15/python%E7%88%AC%E8%99%AB%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%80%E8%A7%88/","excerpt":"正在进行有关爬虫的课程项目，在这里把爬虫过程中遇到的问题、报错、感想等都总结一下","text":"正在进行有关爬虫的课程项目，在这里把爬虫过程中遇到的问题、报错、感想等都总结一下 2020年4月15日 今天想要实现的功能是爬取网易云用户主页的收藏歌单，歌单的具体内容（标签、介绍） 今天处理的问题有：（1）链接跳转以及页面过期问题（2）定位节点以及模拟节点点击问题（3）节点不存在的错误抛出问题 1、链接跳转即页面过期问题： 想要爬取歌单的更多信息，需要跳转到歌单所在的页面，经过的网页源码的分析可以发现，每个表示歌单节点的内容里可以找到关于该歌单的网页地址，通过browser的get再定位就可以跳转到相应的页面。 关于歌单节点的网页分析如下图所示： 直接利用在用户主页获取的收藏歌单节点的列表lists进行元素遍历，然后依次访问每个歌单的链接，会发现报错: StaleElementReferenceExcepti 1selenium.common.exceptions.StaleElementReferenceException: Message: stale eleme 这个错误表示页面内容的过时，即已经跳转到新的页面了，但是lists连接原本的是旧页面，现在获取的都是新页面里面的li的内容，自然找不到想要的内容（自己理解的原理，不一定正确），因而过时错误。 解决方法：在转换页面前，将lists中想要的li节点的属性href重新保存在一个新的列表里，然后遍历新的列表即可。 相关代码： 123456789101112'''lists是之前函数获取的li节点的合集列表'''def get_moreinfo(lists):#获取与歌单信息相关的内容 id0 ='g_iframe' news = []#新的用于保存href的列表 #for li in lists: for i in range(10):#需要另保存在一个列表里，因为会存在过时问题 #newurl = str(li.get_attribute('href'))#这是歌单的网址 newurl = str(lists[i].get_attribute('href'))#这是歌单的网址 news.append(newurl) for i in range(10): browser.get(news[i]) browser.switch_to.frame(id0) 2、定位节点以及节点点击问题问题引入：歌单详细信息的介绍内容经常需要鼠标点击“展开”按钮后，里面所有的内容才能完全的显示出来被爬虫获取，因而需要获取“展开”节点并模拟点击该节点。网页结构如图所示： 定位节点即通过find相关函数，输入节点特有属性值，即可找到节点，目前更常用的是ccs选择器，觉得一目了然，比较简单。节点点击：简单版：获取的节点（WebElement类型）.click（）（更难的还在探索过程中……）出现的问题： 1WebDriverException: Message: Element is not clickable at point (918, 13). Other element would receive the click: &lt;div class=\"xxx\"&gt;&lt;/div&gt; 这个错误目前了解的不是很多，大体的意思是点击这个button的时候，这个单击事件被上层的div给接收了……说明div覆盖在这个button上面。上面的div可能也存在相应的节点，导致点击事件被截获。 解决方法 ：我是再去找节点的特殊性，使节点只能定位到我想操作的那一个节点上，该节点的文本内容只有“展开”，而其他节点可能是“展开XXX”，借助str中的方法endswith(),判断点的后缀是否是“展开”来定位到该节点并click（）打开 相关代码： 123456789'''判断是否有需要展开的内容''' try: check = browser.find_element_by_css_selector('#album-desc-spread') except NoSuchElementException: pass else: if check.text.endswith('展开'): check.click() sleep(2) 3、节点不存在的错误抛出问题 问题引入:不同的歌单有不同的属性。有的歌单可能标签+介绍+介绍展开三者节点都存在，有的可能只存在一个或两个，有的甚至一个节点都不存在。如图所示： 三种节点都有的歌单 有两种节点的歌单 一个节点都没有的歌单 如果获取不存在的节点会报错： 1selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: 解决方法：设置异常抛出，没找到节点不执行任何行为，找到了则输出相关内容，三个节点都要判断。相关代码： 1234567'''输出介绍信息'''try: des = browser.find_element_by_css_selector('#album-desc-more')except NoSuchElementException: passelse: print(des.text) 注意：一开始会出现NameError: name ‘NoSuchElementException’ is not defined的报错，即python内部没有相关错误类，通过引入即可 1from selenium.common.exceptions import NoSuchElementException","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"http://junewang0614.github.io/tags/Python/"}]},{"title":"markdown语法一览","slug":"markdown语法一览","date":"2020-02-23T04:36:43.000Z","updated":"2020-02-23T05:05:32.288Z","comments":true,"path":"2020/02/23/markdown语法一览/","link":"","permalink":"http://junewang0614.github.io/2020/02/23/markdown%E8%AF%AD%E6%B3%95%E4%B8%80%E8%A7%88/","excerpt":"总结自己新get的以及总是记不住的markdown语法","text":"总结自己新get的以及总是记不住的markdown语法 1、添加链接 给文字添加链接 1[链接文字]（链接网址 \"标题\"） 1This is [June's blog](https://junewang0614.github.io/ \"June\") This is June’s blog 插入图片链接 网络连接 1![name](网络链接) 1![凯哥](https://c-ssl.duitang.com/uploads/item/201912/08/20191208152338_gasau.jpg) 本地连接 1![name](本地路径) 2、创建表格 简单 1234name | fun1 | fun2 -|-|-aa | bb | cc |ff | ee | dd | name fun1 fun2 aa bb cc ff ee dd 中等 1234name | 111 | 222 | 333 | 444- | :-: | :-: | :-: | -:aaa | bbb | ccc | ddd | eee| fff | ggg| hhh | iii | 000| name 111 222 333 444 aaa bbb ccc ddd eee fff ggg hhh iii 000 可以居中 复杂 1234name | 111 | 222 | 333 | 444:-: | :-: | :-: | :-: | :-:aaa | bbb | ccc | ddd | eee| fff | ggg| hhh | iii | 000| name 111 222 333 444 aaa bbb ccc ddd eee fff ggg hhh iii 000 总结 -:表示内容和标题栏居右对齐 :-表示内容和标题栏居左对齐 :-:表示内容和标题栏居中对齐 内容和|之间的多余空格会被忽略，每行第一个|和最后一个|可以省略 -的数量至少有一个。","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://junewang0614.github.io/tags/%E5%85%B6%E4%BB%96/"}]},{"title":"dp刷题列表","slug":"dp刷题列表","date":"2020-02-23T04:32:04.000Z","updated":"2020-02-23T04:44:26.976Z","comments":true,"path":"2020/02/23/dp刷题列表/","link":"","permalink":"http://junewang0614.github.io/2020/02/23/dp%E5%88%B7%E9%A2%98%E5%88%97%E8%A1%A8/","excerpt":"将做过的感觉不错的动态规划的题目在这里整理一下","text":"将做过的感觉不错的动态规划的题目在这里整理一下 基础dp 题目 名称 HDU2018 母牛的故事","categories":[],"tags":[{"name":"刷题列表","slug":"刷题列表","permalink":"http://junewang0614.github.io/tags/%E5%88%B7%E9%A2%98%E5%88%97%E8%A1%A8/"}]},{"title":"一些软件安装遇到的问题","slug":"一些软件安装遇到的问题","date":"2020-02-17T13:38:29.000Z","updated":"2020-02-22T07:58:12.054Z","comments":true,"path":"2020/02/17/一些软件安装遇到的问题/","link":"","permalink":"http://junewang0614.github.io/2020/02/17/%E4%B8%80%E4%BA%9B%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"总结一下安装软件的问题以及如何使用一些软件","text":"总结一下安装软件的问题以及如何使用一些软件 1、Anaconda： Prompt 是设置了Anaconda 路径环境变量的命令提示行。 conda 包管理器，各种命令可以创建、查看环境及其安装包的列表 IPython 交互式解释器 Spyder IDE","categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"http://junewang0614.github.io/tags/%E5%85%B6%E4%BB%96/"}]},{"title":"MySQL安装问题总结","slug":"MySQL安装问题总结","date":"2020-02-16T05:53:04.000Z","updated":"2020-03-17T12:34:02.120Z","comments":true,"path":"2020/02/16/MySQL安装问题总结/","link":"","permalink":"http://junewang0614.github.io/2020/02/16/MySQL%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","excerpt":"总结一下在安装MySQL时遇到的一些问题","text":"总结一下在安装MySQL时遇到的一些问题 配置环境变量。 用cmd运行mysql发现没有该指令，是因为没有配置相应环境变量，新建了系统变量MYSQL_HOME其中保存了mysql安装路径（直达bin目录），然后在path变量中同样添加了sql的bin目录路径。其实环境变量挺好配置，一般我失败的原因是因为在配置过程中有很多弹窗都需要点击确认,如果有遗漏就容易配置失败。 有关my.ini文件 在查询各种问题的时候，发现自己的安装目录下没有my.ini文件，一度怀疑自己没安上。。后来一篇博客上提示my.ini文件保存在ProgramData文件夹中，然后复制回了安装目录（Program Files\\MySQL\\MySQL Server x.y）…..这波操作是有点骚。。 my.ini是mysql的配置文件。 再次在cmd中运行mysql命令 再次运行的时候出现的错误问题是 ERROR 1045 (28000): Access denied for user ‘ODBC’@’localhost’ (using password: NO) 好像就是没有进行密码登录，调用指令mysql -u root -p会提示密码输入，输入后即Welcome to the MySQL monitor. mysql的两个 配置参数 basedir 参数：指定了mysql的生成路径，即输入“mysqld install”生成文件datadir 参数：指定了数据库文件的存放路径（也就是今天改了一上午的玩意儿……） 更改数据库文件的存放路径：就是更改datadir参数，注意因为管理员权限，文件不能随便更改，需要在别的地方更改后复制粘贴到相应文件夹中。更改过程需要将mysql服务器关闭。 更改数据库文件存放路径后更重要的是，当前目录下数据库的初始化（实测这种做法相当于恢复出厂设置，之前的数据库操作是没有初始化的，所以慎用，还需继续学习） 更改数据库文件存放路径基本过程（win10管理员模式下）： 关闭数据库服务 1net stop mysql 更改my.ini文件中datadir的配置 移除原mysql服务 1mysqld -remove mysql 初始化data文件，程序会自动创建 1mysql --initialize-insecure 会出现一种报错 Failed to find valid data direatory. 出现这种错误的情况有很多，可能是文件夹路径没有写准确等等。对于我来说，我出现的问题是需要设置user，即 1mysql --initialize-insecure --user=mysql –user的作用是使用哪个用户来运行mysql server，好像其他的用户没有一定的访问权限。 注意 ： –initialize 表示的是初始化，会更改原root的密码 -insecure 表示的是初始化后密码为空，若没有，则会随机生成密码，此时需要及时找到随机生成的密码。 在路径下出现Data文件夹则初始化成功 生成mysql服务 1mysqld --install 启动mysql服务 1net start mysql 修改用户登录密码 实质是操作mysql数据库 12USE mysqlALTER USER &#39;name&#39;@&#39;localhost&#39; IDENTIFIED WITH MYSQL_NATIVE_PASSWORD BY &#39;new password&#39;; –console 可以显示运行情况，会在cmd中显示错误内容，而不需自己再去打开相应错误文件。u1s1，一定要学会看错误文件呀，虽然网上有教程，但毕竟需要具体问题具体解决。 一直以为mysql只能通过命令行来运行，后来查了一下可以用navicat，有图形化的界面可以管理数据库。navicat是需要注册激活，目前最新版15没有找到激活方法，所以用的是12，以下是教程链接： https://blog.csdn.net/niejiangshuai/article/details/87874999","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://junewang0614.github.io/tags/SQL/"}]},{"title":"学习计划-01-22修改","slug":"学习计划-01-22修改","date":"2020-01-22T01:44:01.000Z","updated":"2020-01-22T08:08:12.645Z","comments":true,"path":"2020/01/22/学习计划-01-22修改/","link":"","permalink":"http://junewang0614.github.io/2020/01/22/%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92-01-22%E4%BF%AE%E6%94%B9/","excerpt":"学习计划（20-01-22修改） 回到家里已经好几天了，是时候安排假期的学习了","text":"学习计划（20-01-22修改） 回到家里已经好几天了，是时候安排假期的学习了 首先是关于数据结构与算法，在经过不断挑选深思熟虑之后最终决定先以《数据结构与算法c语言实现》复习数据结构，以《算法基础》来学习算法。 数据结构是上学期的学习内容，但个人感觉掌握的并不扎实，理解的也不到位，重点是自己上学期的代码量实在太少。假期决定整体复习一下数据结构，重点在于各类数据结构的代码实现，其实本想使用c++语言版，但感觉对个人而言还是有一定难度。如果有时间，希望能使用c++实现每种数据结构。 算法在暑假集训有了初步的认识感知，但没有系统学习过，其实想学习的算法的一大目的是因为三月份的蓝桥杯比赛，希望自己能拿到一个好成绩，目前选择《算法基础》入门，回校打算学习《算法》第四版。 下面说一下安排叭： 最多一周，回顾基础的数据结构，包括线性数据结构、树形数据结构、还有其他的一些数据结构，基础操作代码实现。配套辅以洛谷的专题练习，以及leetcode中的相应题目。STL也要学喽。（截至01-31） 《算法基础》，复习数据结构的时候每天阅读一章，一周结束后回来刷题，定量一下每种思想至少7题吧。 然后是编程语言学习，要学的有python和c# Python就是把《py从入门到实践》看完写完 c#目前有电子版《head first c#》看完写完 然后是编程练习 蓝桥杯每天10个题（暂定），难的话每天5个题，3h，晚上写感想 洛谷按照板块刷题 先这样吧2020-01-22 修改","categories":[],"tags":[{"name":"CS","slug":"CS","permalink":"http://junewang0614.github.io/tags/CS/"}]},{"title":"2020-1-17","slug":"2020-1-17","date":"2020-01-17T12:26:10.000Z","updated":"2020-01-17T12:56:03.969Z","comments":true,"path":"2020/01/17/2020-1-17/","link":"","permalink":"http://junewang0614.github.io/2020/01/17/2020-1-17/","excerpt":"2020-1-17小记 腊月二十三， 北方小年，终于踏上回家的征程。在火车上读诗，读一会儿睡一会儿，醒了继续读，好不惬意。从前觉得很长的七个多小时竟也就这样慢慢打发过去了。可能是因为早车吧，难得的周围的座位人很少，旁边的座位从第二站过后倒是再也没有人坐过。几个同省的人上车，方言有一定的熟悉感，向北走的路程中，时不时能看到窗外的雪景，车里暖洋洋的，就这样也就回了家。","text":"2020-1-17小记 腊月二十三， 北方小年，终于踏上回家的征程。在火车上读诗，读一会儿睡一会儿，醒了继续读，好不惬意。从前觉得很长的七个多小时竟也就这样慢慢打发过去了。可能是因为早车吧，难得的周围的座位人很少，旁边的座位从第二站过后倒是再也没有人坐过。几个同省的人上车，方言有一定的熟悉感，向北走的路程中，时不时能看到窗外的雪景，车里暖洋洋的，就这样也就回了家。 小半年了吧，七月底就回了学校，除了自己的房间，自己在家里的痕迹也越来越少。果然啊，没有什么是能留住的，我们能抓住的，其实只有现在。这么简单的道理，竟也是最近才变得清晰起来。和同学讨论存储问题，他问，手机照片那么多，为什么不直接导进硬盘里，我说，还没有云备份。因为从前家里电脑硬盘损坏，也带走了数码相机的记忆。从此备两份，本地电脑与百度网盘。可是当我发出文字我才想到，云盘，也不过是一堆数据罢了，哪一天百度想要关闭网盘了，也不过是留不住罢了。这样看来，云盘仿佛比本地更为脆弱，本地的好歹掌握在自己手中，云上的，早被控制在他人手里罢了。 我真的是一个很容易纠结的人，占有欲强，总想要留住那些想要留住的东西，紧紧握住，后来随着时间的流逝，也被慢慢遗忘在记忆的角落。这个过程自己也想不明白，不过是未来某天的日子里，看到什么熟悉的东西，或是听到什么熟悉的旋律，开启某段尘封已久的记忆，蓦然回首，发现已经走了好远好远。 人生最深沉的悲剧不在于蓦然发生，而在于蓦然回首 早上从书里看到的这一句话。这篇故事讲述的是桂冠诗人弗罗斯特，讲述的主题是选择。选择，一个始终困扰我的词语。我畏惧选择，甚至不愿面对任何选择。从小了说，我厌烦购物时的货比三家，只是毕竟还是普通阶级，能省下的还是要省；能交给他人的选择，更愿意把权利让给对方。往大了说，那些只能由自己做出的选择，总是不到最后一刻不能下定决心，最后一刻的决心也是摇摇晃晃，只要有人稍加劝导，马上能够放弃的那种。为什么这么惧怕选择呢？一则是因为没有真正符合自己心意的选择吧，一方面是因为自己的野心配不上自己的能力，面对的选择也都不是自己想要的，另一方面，不过是自己的贪婪罢了，总觉得最好的选择是自己没有选择的，什么都想要，什么都失掉。 选择的同义词从来都是放弃 选择总会意味着放弃另一或者多方面的选择，因为不甘放弃，所以畏惧选择。 可我们总要在选择中成长，条条大路通罗马，想要去的远方，曲折一点又有何妨。 只是没有什么是留得住的，留不住的就放手吧。抓住只能抓住的现在吧 present is a present 这里是圆。","categories":[],"tags":[{"name":"嗯","slug":"嗯","permalink":"http://junewang0614.github.io/tags/%E5%97%AF/"}]},{"title":"诗的时光书","slug":"诗的时光书","date":"2020-01-17T08:14:49.000Z","updated":"2020-01-17T08:17:34.421Z","comments":true,"path":"2020/01/17/诗的时光书/","link":"","permalink":"http://junewang0614.github.io/2020/01/17/%E8%AF%97%E7%9A%84%E6%97%B6%E5%85%89%E4%B9%A6/","excerpt":"在火车上阅读借了好久的《诗的时光书》，很多诗句挺喜欢的。 摘抄在下面。 伟大的文学尽管会道出悲剧的深刻，但也不会忘记昭示梦想的可贵。","text":"在火车上阅读借了好久的《诗的时光书》，很多诗句挺喜欢的。 摘抄在下面。 伟大的文学尽管会道出悲剧的深刻，但也不会忘记昭示梦想的可贵。 凤凰 ——艾吕雅我是你路上最后一个过客最后一个春天，最后一场雪最后一次求生的战争 我们永远背向西方 一切都披上了曙光的色彩 鲁拜集 第二十九首Into the universe, and why not knowingNor Whence, like Water willy nilly flowingAnd out of it, as Wind along the WasteI know not Whither, willy nilly blowing 我像流水不由自主地来到宇宙不知何来，也不知何由像荒漠之风不由自主地飘去不知何往，也不能停留 [未完待续]","categories":[],"tags":[{"name":"摘抄","slug":"摘抄","permalink":"http://junewang0614.github.io/tags/%E6%91%98%E6%8A%84/"}]},{"title":"写在开头","slug":"写在开头","date":"2020-01-16T03:07:14.000Z","updated":"2020-01-16T11:46:37.992Z","comments":true,"path":"2020/01/16/写在开头/","link":"","permalink":"http://junewang0614.github.io/2020/01/16/%E5%86%99%E5%9C%A8%E5%BC%80%E5%A4%B4/","excerpt":"ABOUT MEJune，六月，也是我的英文名。这是自己搭建的第一个博客，很久没有在本地写markdown文件了，忘记了很多语法，写出来效果是什么也不敢保证。感觉有了博客就像有了自己的一个“云”家一样，什么都可以说，随心所欲。现在的小家只是有一个雏形，很多配置都还没有，慢慢地一点点填满，成为想要的样子。","text":"ABOUT MEJune，六月，也是我的英文名。这是自己搭建的第一个博客，很久没有在本地写markdown文件了，忘记了很多语法，写出来效果是什么也不敢保证。感觉有了博客就像有了自己的一个“云”家一样，什么都可以说，随心所欲。现在的小家只是有一个雏形，很多配置都还没有，慢慢地一点点填满，成为想要的样子。 本来是想借助CSDN的博客，没事写写做题题解，写写学习感悟，写写实验流程，这个博客也是主要以学习为主吧，毕竟自己还是差的太远了，感觉不知不觉又荒废了一年时间，不过慢慢地很多东西倒是明了了起来，只是时间成本有点高，所以未来必须加倍努力。 应该还会再写点自己的一些奇怪的ideas和feelings吧 反正，我的地盘我做主哈哈哈哈 这里是圆。","categories":[],"tags":[{"name":"嗯","slug":"嗯","permalink":"http://junewang0614.github.io/tags/%E5%97%AF/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-01-16T02:16:48.763Z","updated":"2020-01-16T11:46:35.809Z","comments":true,"path":"2020/01/16/hello-world/","link":"","permalink":"http://junewang0614.github.io/2020/01/16/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"http://junewang0614.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"嗯","slug":"嗯","permalink":"http://junewang0614.github.io/tags/%E5%97%AF/"},{"name":"Python","slug":"Python","permalink":"http://junewang0614.github.io/tags/Python/"},{"name":"填词向","slug":"填词向","permalink":"http://junewang0614.github.io/tags/%E5%A1%AB%E8%AF%8D%E5%90%91/"},{"name":"其他","slug":"其他","permalink":"http://junewang0614.github.io/tags/%E5%85%B6%E4%BB%96/"},{"name":"刷题列表","slug":"刷题列表","permalink":"http://junewang0614.github.io/tags/%E5%88%B7%E9%A2%98%E5%88%97%E8%A1%A8/"},{"name":"SQL","slug":"SQL","permalink":"http://junewang0614.github.io/tags/SQL/"},{"name":"CS","slug":"CS","permalink":"http://junewang0614.github.io/tags/CS/"},{"name":"摘抄","slug":"摘抄","permalink":"http://junewang0614.github.io/tags/%E6%91%98%E6%8A%84/"}]}